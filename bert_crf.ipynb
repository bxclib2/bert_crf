{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_crf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Rzc60HW2b549",
        "colab_type": "code",
        "outputId": "382fbfce-4fbe-469f-c451-d67838279baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('treebank')\n",
        "\n",
        "!pip install pytorch-pretrained-bert\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "\n",
        "from nltk.corpus import treebank_chunk\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.123)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.123)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDDTEAqda7_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CnboUoiRrAgG",
        "colab_type": "code",
        "outputId": "ee0397aa-7ce9-4fc5-bd58-087946da0915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpqbiubjkw\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2629054.34B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpqbiubjkw to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpqbiubjkw\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qn7bC0EPc-ci",
        "colab_type": "code",
        "outputId": "0dd2b7ff-ff5e-4c58-a044-0b2dfbdf31c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = [r for r in treebank_chunk.tagged_sents()]\n",
        "train_sent = []\n",
        "for s in train_data:\n",
        "  train_sent.append(\" \".join([t[0] for t in s]))\n",
        "train_sent = [\"[CLS] \"+i+\" [SEP]\" for i in train_sent]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return [tok for tok in self._regexp.split(text) if tok]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xrkm9Q3ilyjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_text = [tokenizer.tokenize(s) for s in train_sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyYUDs74fRKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = [tokenizer.convert_tokens_to_ids(s) for s in tokenized_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xw10qctIrjuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels_pos = []\n",
        "for s,d in zip(tokenized_text,train_data):\n",
        "  temp = []\n",
        "  t_list = d.copy()\n",
        "  for t in s:\n",
        "    \n",
        "    if t == '[CLS]' :\n",
        "      temp.append(\"[START]\")\n",
        "      continue\n",
        "    if t == '[SEP]':\n",
        "      temp.append(\"[STOP]\")\n",
        "      continue      \n",
        "    if t.startswith(\"##\"):\n",
        "      temp.append(\"O\")\n",
        "      continue\n",
        "    if len(t_list)!=0:\n",
        "      if t_list[0][0].lower().startswith(t):\n",
        "        temp.append(t_list[0][1])\n",
        "        t_list.pop(0)\n",
        "        continue\n",
        "    temp.append(\"O\")\n",
        "    continue\n",
        "\n",
        "    \n",
        "  labels_pos.append(temp)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0m6VNgK3KLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_set = []\n",
        "for l in labels_pos:\n",
        "  label_set.extend(l)\n",
        "label_set = set(label_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z8q0rCrRyte8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx2pos = {i:j for i,j in enumerate(label_set)}\n",
        "\n",
        "pos2idx = {j:i for i,j in idx2pos.items()}\n",
        "\n",
        "idx2pos[pos2idx['O']] = idx2pos[0]\n",
        "idx2pos[0] = 'O'\n",
        "\n",
        "pos2idx = {j:i for i,j in idx2pos.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUI11OBbJZFY",
        "colab_type": "code",
        "outputId": "e5492a30-c9ce-4b36-dd32-9d20d9a8504d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "cell_type": "code",
      "source": [
        "pos2idx"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'#': 5,\n",
              " '$': 2,\n",
              " \"''\": 42,\n",
              " '(': 28,\n",
              " ')': 9,\n",
              " ',': 30,\n",
              " '.': 40,\n",
              " ':': 20,\n",
              " 'CC': 45,\n",
              " 'CD': 10,\n",
              " 'DT': 31,\n",
              " 'EX': 15,\n",
              " 'FW': 34,\n",
              " 'IN': 22,\n",
              " 'JJ': 32,\n",
              " 'JJR': 33,\n",
              " 'JJS': 25,\n",
              " 'LS': 11,\n",
              " 'MD': 29,\n",
              " 'NN': 35,\n",
              " 'NNP': 36,\n",
              " 'NNPS': 41,\n",
              " 'NNS': 16,\n",
              " 'O': 0,\n",
              " 'PDT': 6,\n",
              " 'POS': 26,\n",
              " 'PRP': 38,\n",
              " 'PRP$': 24,\n",
              " 'RB': 14,\n",
              " 'RBR': 46,\n",
              " 'RBS': 48,\n",
              " 'RP': 4,\n",
              " 'SYM': 27,\n",
              " 'TO': 1,\n",
              " 'UH': 18,\n",
              " 'VB': 19,\n",
              " 'VBD': 37,\n",
              " 'VBG': 12,\n",
              " 'VBG|NN': 43,\n",
              " 'VBN': 13,\n",
              " 'VBP': 44,\n",
              " 'VBZ': 47,\n",
              " 'WDT': 39,\n",
              " 'WP': 8,\n",
              " 'WP$': 21,\n",
              " 'WRB': 23,\n",
              " '[START]': 3,\n",
              " '[STOP]': 17,\n",
              " '``': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "v7NpqDETzCtw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels_idx = []\n",
        "for s in labels_pos:\n",
        "  temp = []\n",
        "  for p in s:\n",
        "    temp.append(pos2idx[p])\n",
        "  labels_idx.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLNYiePO--GQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data,valid_data = train_data[0:4000],train_data[4000:4009]\n",
        "train_tokenized_text,valid_tokenized_text = tokenized_text[0:4000],tokenized_text[4000:4009]\n",
        "train_indexed_tokens,valid_indexed_tokens = indexed_tokens[0:4000],indexed_tokens[4000:4009]\n",
        "train_labels_pos,valid_labels_pos = labels_pos[0:4000],labels_pos[4000:4009]\n",
        "train_labels_idx,valid_labels_idx = labels_idx[0:4000],labels_idx[4000:4009]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wrFPtyzb8zXD",
        "colab_type": "code",
        "outputId": "394bc5ae-6744-4b9a-c6df-77302d64b8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7525
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "sub_batch = 4\n",
        "acc_time = 4\n",
        "acc_counter = 0\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = 0)#weight = torch.Tensor([1.0,1.0,5.0]))\n",
        "loss_fn.cuda()\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert =  bert.cuda()\n",
        "bert.train()\n",
        "linear = nn.Linear(768,49).cuda()\n",
        "\n",
        "opt = torch.optim.Adam(list(bert.parameters()) + list(linear.parameters()), lr=2e-5)\n",
        "for e in range(EPOCHS):\n",
        "  for b in range(0,len(train_data),sub_batch):\n",
        "    start = b\n",
        "    end = min(len(train_data),b+sub_batch)\n",
        "\n",
        "    tr_X = train_indexed_tokens[start:end]\n",
        "    tr_Y = train_labels_idx[start:end]\n",
        "    tr_X_padded = pad_sequence([torch.Tensor(v) for v in tr_X],batch_first = True).cuda().long()\n",
        "    tr_Y_padded = pad_sequence([torch.Tensor(v) for v in tr_Y],batch_first = True).cuda().long()\n",
        "    segments_tensors = torch.zeros(tr_X_padded.size()).cuda().long()\n",
        "    attention_mask = (tr_X_padded != 0).cuda().long()\n",
        "    encoded_bert, _ = bert(tr_X_padded, segments_tensors,attention_mask,output_all_encoded_layers=False)\n",
        "    pos = linear(encoded_bert)\n",
        "\n",
        "    #print (tr_Y_padded)\n",
        "    \n",
        "    pos = pos.view(-1,pos.size()[-1])\n",
        "    \n",
        "    tr_Y_padded = tr_Y_padded.view(-1)\n",
        "\n",
        "    loss = loss_fn(pos,tr_Y_padded)\n",
        "\n",
        "    loss.backward()\n",
        "    acc_counter = acc_counter+1\n",
        "    if acc_counter == acc_time:\n",
        "      opt.step()\n",
        "      acc_counter = 0\n",
        "      opt.zero_grad()\n",
        "    if b%100 == 0:\n",
        "      print (\"Epoch =\",e,\"loss = \",loss.item())"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp65f9g3ia\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch = 0 loss =  4.024296760559082\n",
            "Epoch = 0 loss =  3.548032283782959\n",
            "Epoch = 0 loss =  3.1204402446746826\n",
            "Epoch = 0 loss =  3.0281245708465576\n",
            "Epoch = 0 loss =  2.545095443725586\n",
            "Epoch = 0 loss =  2.4869539737701416\n",
            "Epoch = 0 loss =  2.415323257446289\n",
            "Epoch = 0 loss =  2.051082134246826\n",
            "Epoch = 0 loss =  1.8237688541412354\n",
            "Epoch = 0 loss =  1.6270627975463867\n",
            "Epoch = 0 loss =  1.298710823059082\n",
            "Epoch = 0 loss =  1.3668063879013062\n",
            "Epoch = 0 loss =  1.041852355003357\n",
            "Epoch = 0 loss =  0.848410964012146\n",
            "Epoch = 0 loss =  0.6554121375083923\n",
            "Epoch = 0 loss =  0.6439481973648071\n",
            "Epoch = 0 loss =  0.7173029184341431\n",
            "Epoch = 0 loss =  0.4560617506504059\n",
            "Epoch = 0 loss =  0.5075008273124695\n",
            "Epoch = 0 loss =  0.36558979749679565\n",
            "Epoch = 0 loss =  0.36088627576828003\n",
            "Epoch = 0 loss =  0.46057695150375366\n",
            "Epoch = 0 loss =  0.6857959628105164\n",
            "Epoch = 0 loss =  0.29613351821899414\n",
            "Epoch = 0 loss =  0.230240598320961\n",
            "Epoch = 0 loss =  0.20240087807178497\n",
            "Epoch = 0 loss =  0.3518933057785034\n",
            "Epoch = 0 loss =  0.26810574531555176\n",
            "Epoch = 0 loss =  0.2902102768421173\n",
            "Epoch = 0 loss =  0.23508203029632568\n",
            "Epoch = 0 loss =  0.3819468319416046\n",
            "Epoch = 0 loss =  0.1506524682044983\n",
            "Epoch = 0 loss =  0.18914532661437988\n",
            "Epoch = 0 loss =  0.18540962040424347\n",
            "Epoch = 0 loss =  0.16096733510494232\n",
            "Epoch = 0 loss =  0.23182493448257446\n",
            "Epoch = 0 loss =  0.19284272193908691\n",
            "Epoch = 0 loss =  0.529190182685852\n",
            "Epoch = 0 loss =  0.12180139869451523\n",
            "Epoch = 0 loss =  0.23563344776630402\n",
            "Epoch = 1 loss =  0.1438918113708496\n",
            "Epoch = 1 loss =  0.10112829506397247\n",
            "Epoch = 1 loss =  0.12074059993028641\n",
            "Epoch = 1 loss =  0.32252058386802673\n",
            "Epoch = 1 loss =  0.1474665105342865\n",
            "Epoch = 1 loss =  0.12478457391262054\n",
            "Epoch = 1 loss =  0.20380638539791107\n",
            "Epoch = 1 loss =  0.15883643925189972\n",
            "Epoch = 1 loss =  0.16569805145263672\n",
            "Epoch = 1 loss =  0.11025650799274445\n",
            "Epoch = 1 loss =  0.16527895629405975\n",
            "Epoch = 1 loss =  0.24372504651546478\n",
            "Epoch = 1 loss =  0.10235827416181564\n",
            "Epoch = 1 loss =  0.08929422497749329\n",
            "Epoch = 1 loss =  0.1996883749961853\n",
            "Epoch = 1 loss =  0.11155015975236893\n",
            "Epoch = 1 loss =  0.1611710488796234\n",
            "Epoch = 1 loss =  0.08397698402404785\n",
            "Epoch = 1 loss =  0.11106859892606735\n",
            "Epoch = 1 loss =  0.10827580094337463\n",
            "Epoch = 1 loss =  0.15764059126377106\n",
            "Epoch = 1 loss =  0.11621829867362976\n",
            "Epoch = 1 loss =  0.2029765248298645\n",
            "Epoch = 1 loss =  0.14370031654834747\n",
            "Epoch = 1 loss =  0.07061463594436646\n",
            "Epoch = 1 loss =  0.03848704695701599\n",
            "Epoch = 1 loss =  0.07976619154214859\n",
            "Epoch = 1 loss =  0.12769335508346558\n",
            "Epoch = 1 loss =  0.0947781577706337\n",
            "Epoch = 1 loss =  0.08542828261852264\n",
            "Epoch = 1 loss =  0.06271463632583618\n",
            "Epoch = 1 loss =  0.02479284629225731\n",
            "Epoch = 1 loss =  0.1061224490404129\n",
            "Epoch = 1 loss =  0.04864487797021866\n",
            "Epoch = 1 loss =  0.05309883877635002\n",
            "Epoch = 1 loss =  0.07230684906244278\n",
            "Epoch = 1 loss =  0.13024544715881348\n",
            "Epoch = 1 loss =  0.3104200065135956\n",
            "Epoch = 1 loss =  0.05410023778676987\n",
            "Epoch = 1 loss =  0.11530905216932297\n",
            "Epoch = 2 loss =  0.06269963830709457\n",
            "Epoch = 2 loss =  0.07895086705684662\n",
            "Epoch = 2 loss =  0.13066962361335754\n",
            "Epoch = 2 loss =  0.12092084437608719\n",
            "Epoch = 2 loss =  0.033539239317178726\n",
            "Epoch = 2 loss =  0.05538288131356239\n",
            "Epoch = 2 loss =  0.125311017036438\n",
            "Epoch = 2 loss =  0.053664110600948334\n",
            "Epoch = 2 loss =  0.09914097934961319\n",
            "Epoch = 2 loss =  0.0432320162653923\n",
            "Epoch = 2 loss =  0.10728529840707779\n",
            "Epoch = 2 loss =  0.20485927164554596\n",
            "Epoch = 2 loss =  0.05845601111650467\n",
            "Epoch = 2 loss =  0.03869733214378357\n",
            "Epoch = 2 loss =  0.08996088057756424\n",
            "Epoch = 2 loss =  0.06912938505411148\n",
            "Epoch = 2 loss =  0.12094049155712128\n",
            "Epoch = 2 loss =  0.06790520995855331\n",
            "Epoch = 2 loss =  0.0923437625169754\n",
            "Epoch = 2 loss =  0.055038657039403915\n",
            "Epoch = 2 loss =  0.0692448690533638\n",
            "Epoch = 2 loss =  0.0691981315612793\n",
            "Epoch = 2 loss =  0.1246221587061882\n",
            "Epoch = 2 loss =  0.1010463535785675\n",
            "Epoch = 2 loss =  0.04721645638346672\n",
            "Epoch = 2 loss =  0.03232492133975029\n",
            "Epoch = 2 loss =  0.06139175966382027\n",
            "Epoch = 2 loss =  0.10121607035398483\n",
            "Epoch = 2 loss =  0.028156397864222527\n",
            "Epoch = 2 loss =  0.02669002115726471\n",
            "Epoch = 2 loss =  0.04323761165142059\n",
            "Epoch = 2 loss =  0.017646703869104385\n",
            "Epoch = 2 loss =  0.09939341992139816\n",
            "Epoch = 2 loss =  0.023657353594899178\n",
            "Epoch = 2 loss =  0.030698468908667564\n",
            "Epoch = 2 loss =  0.05978759378194809\n",
            "Epoch = 2 loss =  0.09071366488933563\n",
            "Epoch = 2 loss =  0.11395490169525146\n",
            "Epoch = 2 loss =  0.0377986915409565\n",
            "Epoch = 2 loss =  0.08274564146995544\n",
            "Epoch = 3 loss =  0.06587856262922287\n",
            "Epoch = 3 loss =  0.03248530998826027\n",
            "Epoch = 3 loss =  0.09150505065917969\n",
            "Epoch = 3 loss =  0.04579480364918709\n",
            "Epoch = 3 loss =  0.05308077856898308\n",
            "Epoch = 3 loss =  0.041087035089731216\n",
            "Epoch = 3 loss =  0.06120655685663223\n",
            "Epoch = 3 loss =  0.04564882442355156\n",
            "Epoch = 3 loss =  0.08688787370920181\n",
            "Epoch = 3 loss =  0.02515873685479164\n",
            "Epoch = 3 loss =  0.10280226171016693\n",
            "Epoch = 3 loss =  0.16239066421985626\n",
            "Epoch = 3 loss =  0.03767247125506401\n",
            "Epoch = 3 loss =  0.022707264870405197\n",
            "Epoch = 3 loss =  0.05817025527358055\n",
            "Epoch = 3 loss =  0.057342734187841415\n",
            "Epoch = 3 loss =  0.09853878617286682\n",
            "Epoch = 3 loss =  0.018613377586007118\n",
            "Epoch = 3 loss =  0.09185303002595901\n",
            "Epoch = 3 loss =  0.049054186791181564\n",
            "Epoch = 3 loss =  0.05182125046849251\n",
            "Epoch = 3 loss =  0.03197937458753586\n",
            "Epoch = 3 loss =  0.10748052597045898\n",
            "Epoch = 3 loss =  0.11461112648248672\n",
            "Epoch = 3 loss =  0.05233977735042572\n",
            "Epoch = 3 loss =  0.012243992649018764\n",
            "Epoch = 3 loss =  0.05137918144464493\n",
            "Epoch = 3 loss =  0.06980381160974503\n",
            "Epoch = 3 loss =  0.023954104632139206\n",
            "Epoch = 3 loss =  0.023876890540122986\n",
            "Epoch = 3 loss =  0.017075849696993828\n",
            "Epoch = 3 loss =  0.0090348981320858\n",
            "Epoch = 3 loss =  0.08107373863458633\n",
            "Epoch = 3 loss =  0.020040808245539665\n",
            "Epoch = 3 loss =  0.01586395874619484\n",
            "Epoch = 3 loss =  0.03498511761426926\n",
            "Epoch = 3 loss =  0.02749418467283249\n",
            "Epoch = 3 loss =  0.07443966716527939\n",
            "Epoch = 3 loss =  0.042870037257671356\n",
            "Epoch = 3 loss =  0.0449146069586277\n",
            "Epoch = 4 loss =  0.0603482611477375\n",
            "Epoch = 4 loss =  0.009386535733938217\n",
            "Epoch = 4 loss =  0.07918695360422134\n",
            "Epoch = 4 loss =  0.03060937114059925\n",
            "Epoch = 4 loss =  0.013871889561414719\n",
            "Epoch = 4 loss =  0.04350302368402481\n",
            "Epoch = 4 loss =  0.05773892253637314\n",
            "Epoch = 4 loss =  0.02144729532301426\n",
            "Epoch = 4 loss =  0.06340280175209045\n",
            "Epoch = 4 loss =  0.026439571753144264\n",
            "Epoch = 4 loss =  0.060459960252046585\n",
            "Epoch = 4 loss =  0.1281869262456894\n",
            "Epoch = 4 loss =  0.035668373107910156\n",
            "Epoch = 4 loss =  0.020823651924729347\n",
            "Epoch = 4 loss =  0.03805266320705414\n",
            "Epoch = 4 loss =  0.02283986657857895\n",
            "Epoch = 4 loss =  0.08532575517892838\n",
            "Epoch = 4 loss =  0.01995006389915943\n",
            "Epoch = 4 loss =  0.0939481258392334\n",
            "Epoch = 4 loss =  0.05903299152851105\n",
            "Epoch = 4 loss =  0.030583690851926804\n",
            "Epoch = 4 loss =  0.013569160364568233\n",
            "Epoch = 4 loss =  0.08956001698970795\n",
            "Epoch = 4 loss =  0.09464312344789505\n",
            "Epoch = 4 loss =  0.024501902982592583\n",
            "Epoch = 4 loss =  0.008140303194522858\n",
            "Epoch = 4 loss =  0.014594119973480701\n",
            "Epoch = 4 loss =  0.07624989002943039\n",
            "Epoch = 4 loss =  0.01661674864590168\n",
            "Epoch = 4 loss =  0.014396801590919495\n",
            "Epoch = 4 loss =  0.015022503212094307\n",
            "Epoch = 4 loss =  0.011773655191063881\n",
            "Epoch = 4 loss =  0.08217703551054001\n",
            "Epoch = 4 loss =  0.016538826748728752\n",
            "Epoch = 4 loss =  0.010963461361825466\n",
            "Epoch = 4 loss =  0.021694323047995567\n",
            "Epoch = 4 loss =  0.012659951113164425\n",
            "Epoch = 4 loss =  0.0259674284607172\n",
            "Epoch = 4 loss =  0.007869361899793148\n",
            "Epoch = 4 loss =  0.03195420652627945\n",
            "Epoch = 5 loss =  0.047104936093091965\n",
            "Epoch = 5 loss =  0.006814684718847275\n",
            "Epoch = 5 loss =  0.07552902400493622\n",
            "Epoch = 5 loss =  0.18485359847545624\n",
            "Epoch = 5 loss =  0.008351175114512444\n",
            "Epoch = 5 loss =  0.06498578190803528\n",
            "Epoch = 5 loss =  0.016351154074072838\n",
            "Epoch = 5 loss =  0.019713878631591797\n",
            "Epoch = 5 loss =  0.05061522498726845\n",
            "Epoch = 5 loss =  0.009052786976099014\n",
            "Epoch = 5 loss =  0.02026381716132164\n",
            "Epoch = 5 loss =  0.09456220269203186\n",
            "Epoch = 5 loss =  0.021206531673669815\n",
            "Epoch = 5 loss =  0.014785053208470345\n",
            "Epoch = 5 loss =  0.03799201920628548\n",
            "Epoch = 5 loss =  0.013470358215272427\n",
            "Epoch = 5 loss =  0.06765318661928177\n",
            "Epoch = 5 loss =  0.057446833699941635\n",
            "Epoch = 5 loss =  0.035179153084754944\n",
            "Epoch = 5 loss =  0.02041620947420597\n",
            "Epoch = 5 loss =  0.018291357904672623\n",
            "Epoch = 5 loss =  0.024201570078730583\n",
            "Epoch = 5 loss =  0.09266343712806702\n",
            "Epoch = 5 loss =  0.08566729724407196\n",
            "Epoch = 5 loss =  0.01770014315843582\n",
            "Epoch = 5 loss =  0.010930709540843964\n",
            "Epoch = 5 loss =  0.01687835156917572\n",
            "Epoch = 5 loss =  0.06320051848888397\n",
            "Epoch = 5 loss =  0.007344180718064308\n",
            "Epoch = 5 loss =  0.008674174547195435\n",
            "Epoch = 5 loss =  0.023022988811135292\n",
            "Epoch = 5 loss =  0.008565719239413738\n",
            "Epoch = 5 loss =  0.03342433273792267\n",
            "Epoch = 5 loss =  0.027781888842582703\n",
            "Epoch = 5 loss =  0.009335878305137157\n",
            "Epoch = 5 loss =  0.015505494549870491\n",
            "Epoch = 5 loss =  0.005320020020008087\n",
            "Epoch = 5 loss =  0.020568309351801872\n",
            "Epoch = 5 loss =  0.015066920779645443\n",
            "Epoch = 5 loss =  0.03251294791698456\n",
            "Epoch = 6 loss =  0.07370709627866745\n",
            "Epoch = 6 loss =  0.005062722135335207\n",
            "Epoch = 6 loss =  0.09116442501544952\n",
            "Epoch = 6 loss =  0.020213494077324867\n",
            "Epoch = 6 loss =  0.005035839509218931\n",
            "Epoch = 6 loss =  0.008887480944395065\n",
            "Epoch = 6 loss =  0.008532281965017319\n",
            "Epoch = 6 loss =  0.014594481326639652\n",
            "Epoch = 6 loss =  0.008520947769284248\n",
            "Epoch = 6 loss =  0.026204165071249008\n",
            "Epoch = 6 loss =  0.016490381211042404\n",
            "Epoch = 6 loss =  0.07370492070913315\n",
            "Epoch = 6 loss =  0.017762351781129837\n",
            "Epoch = 6 loss =  0.01040708925575018\n",
            "Epoch = 6 loss =  0.039327941834926605\n",
            "Epoch = 6 loss =  0.017511559650301933\n",
            "Epoch = 6 loss =  0.06651917099952698\n",
            "Epoch = 6 loss =  0.027710601687431335\n",
            "Epoch = 6 loss =  0.07483983039855957\n",
            "Epoch = 6 loss =  0.03300486505031586\n",
            "Epoch = 6 loss =  0.015010297298431396\n",
            "Epoch = 6 loss =  0.0174117349088192\n",
            "Epoch = 6 loss =  0.06547407805919647\n",
            "Epoch = 6 loss =  0.08894342184066772\n",
            "Epoch = 6 loss =  0.012499763630330563\n",
            "Epoch = 6 loss =  0.006638494320213795\n",
            "Epoch = 6 loss =  0.0072069307789206505\n",
            "Epoch = 6 loss =  0.06738893687725067\n",
            "Epoch = 6 loss =  0.006189581472426653\n",
            "Epoch = 6 loss =  0.007605999708175659\n",
            "Epoch = 6 loss =  0.007052282337099314\n",
            "Epoch = 6 loss =  0.006909826770424843\n",
            "Epoch = 6 loss =  0.004240983631461859\n",
            "Epoch = 6 loss =  0.028559071943163872\n",
            "Epoch = 6 loss =  0.011835750192403793\n",
            "Epoch = 6 loss =  0.016875553876161575\n",
            "Epoch = 6 loss =  0.0045786043629050255\n",
            "Epoch = 6 loss =  0.05592517927289009\n",
            "Epoch = 6 loss =  0.005922143347561359\n",
            "Epoch = 6 loss =  0.015913361683487892\n",
            "Epoch = 7 loss =  0.011558008380234241\n",
            "Epoch = 7 loss =  0.0038214398082345724\n",
            "Epoch = 7 loss =  0.09110893309116364\n",
            "Epoch = 7 loss =  0.0325264148414135\n",
            "Epoch = 7 loss =  0.031099513173103333\n",
            "Epoch = 7 loss =  0.0048429109156131744\n",
            "Epoch = 7 loss =  0.00835547037422657\n",
            "Epoch = 7 loss =  0.00784590095281601\n",
            "Epoch = 7 loss =  0.020931072533130646\n",
            "Epoch = 7 loss =  0.00653244461864233\n",
            "Epoch = 7 loss =  0.004826226271688938\n",
            "Epoch = 7 loss =  0.048309508711099625\n",
            "Epoch = 7 loss =  0.010281122289597988\n",
            "Epoch = 7 loss =  0.006190350744873285\n",
            "Epoch = 7 loss =  0.02181093394756317\n",
            "Epoch = 7 loss =  0.003932781983166933\n",
            "Epoch = 7 loss =  0.054595571011304855\n",
            "Epoch = 7 loss =  0.01912490651011467\n",
            "Epoch = 7 loss =  0.06129816547036171\n",
            "Epoch = 7 loss =  0.019047899171710014\n",
            "Epoch = 7 loss =  0.011041144840419292\n",
            "Epoch = 7 loss =  0.05330076068639755\n",
            "Epoch = 7 loss =  0.07023091614246368\n",
            "Epoch = 7 loss =  0.06157165393233299\n",
            "Epoch = 7 loss =  0.01009149756282568\n",
            "Epoch = 7 loss =  0.004180447664111853\n",
            "Epoch = 7 loss =  0.009889069944620132\n",
            "Epoch = 7 loss =  0.02699463814496994\n",
            "Epoch = 7 loss =  0.005501897539943457\n",
            "Epoch = 7 loss =  0.004353702068328857\n",
            "Epoch = 7 loss =  0.013999644666910172\n",
            "Epoch = 7 loss =  0.0029613461811095476\n",
            "Epoch = 7 loss =  0.004313698038458824\n",
            "Epoch = 7 loss =  0.009925630874931812\n",
            "Epoch = 7 loss =  0.004911385476589203\n",
            "Epoch = 7 loss =  0.01201863493770361\n",
            "Epoch = 7 loss =  0.01057170145213604\n",
            "Epoch = 7 loss =  0.015033150091767311\n",
            "Epoch = 7 loss =  0.004395115654915571\n",
            "Epoch = 7 loss =  0.005418869666755199\n",
            "Epoch = 8 loss =  0.009079602546989918\n",
            "Epoch = 8 loss =  0.002910036826506257\n",
            "Epoch = 8 loss =  0.0330536849796772\n",
            "Epoch = 8 loss =  0.011211086995899677\n",
            "Epoch = 8 loss =  0.0029982642736285925\n",
            "Epoch = 8 loss =  0.004574414808303118\n",
            "Epoch = 8 loss =  0.02164738066494465\n",
            "Epoch = 8 loss =  0.007586246356368065\n",
            "Epoch = 8 loss =  0.0110845435410738\n",
            "Epoch = 8 loss =  0.003857203060761094\n",
            "Epoch = 8 loss =  0.004725482314825058\n",
            "Epoch = 8 loss =  0.015176604501903057\n",
            "Epoch = 8 loss =  0.017661383375525475\n",
            "Epoch = 8 loss =  0.0064314319752156734\n",
            "Epoch = 8 loss =  0.030652187764644623\n",
            "Epoch = 8 loss =  0.004041116684675217\n",
            "Epoch = 8 loss =  0.05582891032099724\n",
            "Epoch = 8 loss =  0.004158509895205498\n",
            "Epoch = 8 loss =  0.05744463950395584\n",
            "Epoch = 8 loss =  0.005562786478549242\n",
            "Epoch = 8 loss =  0.008755059912800789\n",
            "Epoch = 8 loss =  0.028393983840942383\n",
            "Epoch = 8 loss =  0.05615663155913353\n",
            "Epoch = 8 loss =  0.044835738837718964\n",
            "Epoch = 8 loss =  0.008323351852595806\n",
            "Epoch = 8 loss =  0.00345514970831573\n",
            "Epoch = 8 loss =  0.004664147738367319\n",
            "Epoch = 8 loss =  0.015862690284848213\n",
            "Epoch = 8 loss =  0.005167830269783735\n",
            "Epoch = 8 loss =  0.004408515989780426\n",
            "Epoch = 8 loss =  0.009800144471228123\n",
            "Epoch = 8 loss =  0.04140307754278183\n",
            "Epoch = 8 loss =  0.011079355143010616\n",
            "Epoch = 8 loss =  0.006253817118704319\n",
            "Epoch = 8 loss =  0.003766907611861825\n",
            "Epoch = 8 loss =  0.027472758665680885\n",
            "Epoch = 8 loss =  0.0033987252973020077\n",
            "Epoch = 8 loss =  0.02730473130941391\n",
            "Epoch = 8 loss =  0.003677962813526392\n",
            "Epoch = 8 loss =  0.009704982861876488\n",
            "Epoch = 9 loss =  0.014462834224104881\n",
            "Epoch = 9 loss =  0.0024706856347620487\n",
            "Epoch = 9 loss =  0.056934796273708344\n",
            "Epoch = 9 loss =  0.04179725795984268\n",
            "Epoch = 9 loss =  0.0046710968017578125\n",
            "Epoch = 9 loss =  0.005733952857553959\n",
            "Epoch = 9 loss =  0.008400266058743\n",
            "Epoch = 9 loss =  0.008917342871427536\n",
            "Epoch = 9 loss =  0.0038408637046813965\n",
            "Epoch = 9 loss =  0.002943543717265129\n",
            "Epoch = 9 loss =  0.007514266762882471\n",
            "Epoch = 9 loss =  0.006626149173825979\n",
            "Epoch = 9 loss =  0.004459229297935963\n",
            "Epoch = 9 loss =  0.006101612467318773\n",
            "Epoch = 9 loss =  0.028226014226675034\n",
            "Epoch = 9 loss =  0.003148904535919428\n",
            "Epoch = 9 loss =  0.032965853810310364\n",
            "Epoch = 9 loss =  0.004687596578150988\n",
            "Epoch = 9 loss =  0.0323469340801239\n",
            "Epoch = 9 loss =  0.025044448673725128\n",
            "Epoch = 9 loss =  0.003797121811658144\n",
            "Epoch = 9 loss =  0.0035678280983120203\n",
            "Epoch = 9 loss =  0.03921866416931152\n",
            "Epoch = 9 loss =  0.027976345270872116\n",
            "Epoch = 9 loss =  0.006197547074407339\n",
            "Epoch = 9 loss =  0.0029395746532827616\n",
            "Epoch = 9 loss =  0.01769992895424366\n",
            "Epoch = 9 loss =  0.0077688321471214294\n",
            "Epoch = 9 loss =  0.002909052884206176\n",
            "Epoch = 9 loss =  0.030675366520881653\n",
            "Epoch = 9 loss =  0.005359038710594177\n",
            "Epoch = 9 loss =  0.0018751764437183738\n",
            "Epoch = 9 loss =  0.002028106013312936\n",
            "Epoch = 9 loss =  0.007713169325143099\n",
            "Epoch = 9 loss =  0.0039211539551615715\n",
            "Epoch = 9 loss =  0.0361110121011734\n",
            "Epoch = 9 loss =  0.003957115579396486\n",
            "Epoch = 9 loss =  0.00804040115326643\n",
            "Epoch = 9 loss =  0.0030985698103904724\n",
            "Epoch = 9 loss =  0.0071071055717766285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoE2v8anPGRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e06e6295-e92d-4b9e-e8b6-bdac01927fb9"
      },
      "cell_type": "code",
      "source": [
        "valid_trX = pad_sequence([torch.Tensor(v) for v in valid_indexed_tokens],batch_first = True).cuda().long()\n",
        "valid_trY = pad_sequence([torch.Tensor(v) for v in valid_labels_idx],batch_first = True).cuda().long()\n",
        "\n",
        "with torch.no_grad():\n",
        "    bert.eval()\n",
        "    segments_tensors = torch.zeros(valid_trX.size()).cuda().long()\n",
        "    attention_mask = (valid_trX != 0).cuda().long()\n",
        "    encoded_bert, _ = bert(valid_trX, segments_tensors,attention_mask,output_all_encoded_layers=False)\n",
        "    \n",
        "    pos = linear(encoded_bert).view(-1,pos.shape[-1])\n",
        "    valid_trY = valid_trY.view(-1)\n",
        "    pred_trY = torch.argmax(pos,dim = -1)\n",
        "    \n",
        "print (\"Acc = \",(valid_trY[valid_trY!=0] == pred_trY[valid_trY!=0]).float().mean())"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc =  tensor(0.9839, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nf-CEXgtzlF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "ee7cf008-feb8-47f2-e96f-ec7c78527871"
      },
      "cell_type": "code",
      "source": [
        "valid_trY = valid_trY.view(9,49)\n",
        "pred_trY = pred_trY.view(9,49)\n",
        "for i in range(9):\n",
        "  for j in range(49):\n",
        "    if valid_trY[i,j] != 0:\n",
        "      if valid_trY[i,j] != pred_trY[i,j]:\n",
        "        print (i,j)\n",
        "        print (pred_trY[i,j],valid_trY[i,j])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 9\n",
            "tensor(37, device='cuda:0') tensor(13, device='cuda:0')\n",
            "4 20\n",
            "tensor(32, device='cuda:0') tensor(36, device='cuda:0')\n",
            "4 21\n",
            "tensor(35, device='cuda:0') tensor(36, device='cuda:0')\n",
            "4 33\n",
            "tensor(35, device='cuda:0') tensor(32, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrtDFO9EyEgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1799
        },
        "outputId": "a087733e-77c1-4770-e5d8-5d0af25aa2ea"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(pred_trY.view(9,-1).cpu().numpy()).applymap(lambda x: idx2pos[x]).loc[0],pd.DataFrame(valid_trY.view(9,-1).cpu().numpy()).applymap(lambda x: idx2pos[x]).loc[0]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0     [START]\n",
              " 1          CC\n",
              " 2          DT\n",
              " 3          CD\n",
              " 4          JJ\n",
              " 5         NNS\n",
              " 6           ,\n",
              " 7         VBG\n",
              " 8          TO\n",
              " 9          DT\n",
              " 10         NN\n",
              " 11         IN\n",
              " 12        NNP\n",
              " 13          .\n",
              " 14        NNP\n",
              " 15        NNP\n",
              " 16          (\n",
              " 17        NNP\n",
              " 18          .\n",
              " 19          ,\n",
              " 20        NNP\n",
              " 21          .\n",
              " 22          )\n",
              " 23          ,\n",
              " 24        VBD\n",
              " 25         IN\n",
              " 26         DT\n",
              " 27         JJ\n",
              " 28         NN\n",
              " 29         IN\n",
              " 30         DT\n",
              " 31         NN\n",
              " 32         ``\n",
              " 33         ``\n",
              " 34        VBZ\n",
              " 35         DT\n",
              " 36         JJ\n",
              " 37         NN\n",
              " 38         TO\n",
              " 39         VB\n",
              " 40         DT\n",
              " 41         JJ\n",
              " 42          :\n",
              " 43         NN\n",
              " 44         NN\n",
              " 45          .\n",
              " 46     [STOP]\n",
              " 47         NN\n",
              " 48         NN\n",
              " Name: 0, dtype: object, 0     [START]\n",
              " 1          CC\n",
              " 2          DT\n",
              " 3          CD\n",
              " 4          JJ\n",
              " 5         NNS\n",
              " 6           ,\n",
              " 7         VBG\n",
              " 8          TO\n",
              " 9          DT\n",
              " 10         NN\n",
              " 11         IN\n",
              " 12        NNP\n",
              " 13          O\n",
              " 14        NNP\n",
              " 15        NNP\n",
              " 16          (\n",
              " 17        NNP\n",
              " 18          O\n",
              " 19          ,\n",
              " 20        NNP\n",
              " 21          O\n",
              " 22          )\n",
              " 23          ,\n",
              " 24        VBD\n",
              " 25         IN\n",
              " 26         DT\n",
              " 27         JJ\n",
              " 28         NN\n",
              " 29         IN\n",
              " 30         DT\n",
              " 31         NN\n",
              " 32         ``\n",
              " 33          O\n",
              " 34        VBZ\n",
              " 35         DT\n",
              " 36         JJ\n",
              " 37         NN\n",
              " 38         TO\n",
              " 39         VB\n",
              " 40         DT\n",
              " 41         JJ\n",
              " 42          O\n",
              " 43          O\n",
              " 44         NN\n",
              " 45          .\n",
              " 46     [STOP]\n",
              " 47          O\n",
              " 48          O\n",
              " Name: 0, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "WSMsx58loBAL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CRF(nn.Module):\n",
        "    def __init__(self,start_tag_idx,end_tag_idx,tags_num = 49,batch_size = 4):\n",
        "        super(CRF,self).__init__()\n",
        "        self.tags_num = tags_num\n",
        "        self.transitions = nn.Parameter(torch.ones(tags_num,tags_num)) # 0 postion is for \"O\" so no one can use it\n",
        "        self.start_tag_idx = start_tag_idx\n",
        "        self.end_tag_idx = end_tag_idx\n",
        "        self.batch_size = batch_size\n",
        "        self.register_buffer('last_tag', torch.ones(self.batch_size).long())\n",
        "        \n",
        "           \n",
        "        nn.init.uniform_(self.transitions, -0.01, 0.01)\n",
        "\n",
        "        self.transitions.data[:, start_tag_idx] = -100000.0\n",
        "\n",
        "        self.transitions.data[end_tag_idx, :] = -100000.0\n",
        "\n",
        "    def score_sentence(self,batch_feat, batch_tag):\n",
        "      \n",
        "        self.batch_size, self.seq_length, self.hidden_dim = batch_feat.size()\n",
        "        \n",
        "        score = batch_feat[:,0,self.start_tag_idx].view(-1)\n",
        "        \n",
        "        nn.init.ones_(self.last_tag)\n",
        "        \n",
        "        self.last_tag = self.last_tag*self.start_tag_idx\n",
        "        \n",
        "        mask = (batch_tag != 0).float()\n",
        "\n",
        "        for i in range(1,self.seq_length):\n",
        "\n",
        "          score = score + batch_feat[:,i].gather(dim = 1,index = batch_tag[:,i].unsqueeze(1)).view(-1)*mask[:,i].view(-1)\n",
        "\n",
        "\n",
        "          score = score + self.transitions[self.last_tag,batch_tag[:,i]].view(-1)*mask[:,i].view(-1)\n",
        "          \n",
        "          self.last_tag = torch.where(mask[:,i].byte().view(-1), batch_tag[:,i].view(-1), self.last_tag.view(-1))\n",
        "                    \n",
        "        return score\n",
        "          \n",
        "          \n",
        "          \n",
        "        \n",
        "    def forward(self,batch_feat, batch_tag):\n",
        "      \n",
        "        score = self.score_sentence(batch_feat,batch_tag)\n",
        "        \n",
        "        partition = self.compute_log_partition(batch_feat,(batch_tag!=0))\n",
        "        \n",
        "        return partition - score\n",
        "        \n",
        "    def compute_log_partition(self,batch_feat,mask):\n",
        "      \n",
        "        mask = mask.float()\n",
        "      \n",
        "        self.batch_size, self.seq_length, self.hidden_dim = batch_feat.size()\n",
        "        \n",
        "        alphas = batch_feat[:, 0,:]\n",
        "        \n",
        "        for i in range(1, self.seq_length):\n",
        "             \n",
        "          \n",
        "            alphas = alphas.unsqueeze(2)\n",
        "\n",
        "\n",
        "            e_scores = batch_feat[:, i, :].unsqueeze(1)\n",
        "\n",
        "            t_scores = self.transitions[:, :].unsqueeze(0)\n",
        "\n",
        "            scores = e_scores + t_scores + alphas\n",
        "\n",
        "            new_alphas = torch.logsumexp(scores, dim=1)\n",
        "            \n",
        "            alphas = alphas.squeeze()\n",
        "\n",
        "            mask_t = mask[:, i].unsqueeze(-1)\n",
        "            \n",
        "            alphas = mask_t * new_alphas + (1 - mask_t) * alphas\n",
        "\n",
        "        return torch.logsumexp(alphas, dim=1)\n",
        "      \n",
        "    def decode(self,batch_feat,mask):\n",
        "      \n",
        "        mask = mask.float()\n",
        "      \n",
        "        self.batch_size, self.seq_length, self.hidden_dim = batch_feat.size()\n",
        "        \n",
        "        alphas = batch_feat[:, 0,:]\n",
        "        \n",
        "        history = []\n",
        "        \n",
        "        for i in range(1, self.seq_length):\n",
        "             \n",
        "          \n",
        "            alphas = alphas.unsqueeze(2)\n",
        "\n",
        "\n",
        "            e_scores = batch_feat[:, i, :].unsqueeze(1)\n",
        "\n",
        "            t_scores = self.transitions[:, :].unsqueeze(0)\n",
        "\n",
        "            scores = e_scores + t_scores + alphas\n",
        "\n",
        "            new_alphas,idx = torch.max(scores, dim=1)\n",
        "            \n",
        "            alphas = alphas.squeeze()\n",
        "\n",
        "            mask_t = mask[:, i].unsqueeze(-1)\n",
        "            \n",
        "            alphas = mask_t * new_alphas + (1 - mask_t) * alphas\n",
        "            \n",
        "            history.append(idx)\n",
        "\n",
        "        best_tags_list = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            end_blank = True\n",
        "            _, best_last_tag = alphas[i].max(dim=0)\n",
        "            best_tags = [best_last_tag.item()]\n",
        "\n",
        "            for p,hist in enumerate(reversed(history)): #use end_blank  to find the last valid pos, if directly use the max in a unvalid pos will cause a bug\n",
        "                if mask[i,self.seq_length - 1 - p] == 1.0:\n",
        "                  end_blank = False\n",
        "                if end_blank:\n",
        "                  best_tags.append(best_tags[-1])\n",
        "                else:\n",
        "                  best_last_tag = hist[i][best_tags[-1]]\n",
        "                  best_tags.append(best_last_tag.item())\n",
        "\n",
        "            # Reverse the order because we start from the last timestep\n",
        "            best_tags.reverse()\n",
        "            best_tags_list.append(best_tags)\n",
        "\n",
        "        return torch.LongTensor(best_tags_list).to(batch_feat.device)\n",
        "        \n",
        "        \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBA6_o7xofX9",
        "colab_type": "code",
        "outputId": "02a122cd-561b-4dc5-b925-d26ee5036bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7525
        }
      },
      "cell_type": "code",
      "source": [
        "crf = CRF(pos2idx[\"[START]\"],pos2idx[\"[STOP]\"])\n",
        "crf = crf.cuda()\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "sub_batch = 4\n",
        "acc_time = 4\n",
        "acc_counter = 0\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = 0)#weight = torch.Tensor([1.0,1.0,5.0]))\n",
        "loss_fn.cuda()\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert =  bert.cuda()\n",
        "bert.train()\n",
        "linear = nn.Linear(768,49).cuda()\n",
        "\n",
        "opt = torch.optim.Adam(list(bert.parameters()) +list(crf.parameters()) + list(linear.parameters()), lr=2e-5)\n",
        "for e in range(EPOCHS):\n",
        "  for b in range(0,len(train_data),sub_batch):\n",
        "    start = b\n",
        "    end = min(len(train_data),b+sub_batch)\n",
        "\n",
        "    tr_X = train_indexed_tokens[start:end]\n",
        "    tr_Y = train_labels_idx[start:end]\n",
        "    tr_X_padded = pad_sequence([torch.Tensor(v) for v in tr_X],batch_first = True).cuda().long()\n",
        "    tr_Y_padded = pad_sequence([torch.Tensor(v) for v in tr_Y],batch_first = True).cuda().long()\n",
        "    segments_tensors = torch.zeros(tr_X_padded.size()).cuda().long()\n",
        "    attention_mask = (tr_X_padded != 0).cuda().long()\n",
        "    encoded_bert, _ = bert(tr_X_padded, segments_tensors,attention_mask,output_all_encoded_layers=False)\n",
        "    pos = linear(encoded_bert)\n",
        "\n",
        "    loss = crf(pos,tr_Y_padded).mean()\n",
        "\n",
        "    loss.backward()\n",
        "    acc_counter = acc_counter+1\n",
        "    if acc_counter == acc_time:\n",
        "      opt.step()\n",
        "      acc_counter = 0\n",
        "      opt.zero_grad()\n",
        "    if b%100 == 0:\n",
        "      print (\"Epoch =\",e,\"loss = \",loss.item())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpqrb_74uj\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch = 0 loss =  97.3095703125\n",
            "Epoch = 0 loss =  98.65769958496094\n",
            "Epoch = 0 loss =  79.9422836303711\n",
            "Epoch = 0 loss =  70.08186340332031\n",
            "Epoch = 0 loss =  45.1622314453125\n",
            "Epoch = 0 loss =  58.75858688354492\n",
            "Epoch = 0 loss =  70.2684326171875\n",
            "Epoch = 0 loss =  40.951026916503906\n",
            "Epoch = 0 loss =  28.378543853759766\n",
            "Epoch = 0 loss =  34.17486572265625\n",
            "Epoch = 0 loss =  31.145313262939453\n",
            "Epoch = 0 loss =  37.14376449584961\n",
            "Epoch = 0 loss =  21.612152099609375\n",
            "Epoch = 0 loss =  21.580501556396484\n",
            "Epoch = 0 loss =  19.03482437133789\n",
            "Epoch = 0 loss =  8.3436918258667\n",
            "Epoch = 0 loss =  26.33484649658203\n",
            "Epoch = 0 loss =  7.416454315185547\n",
            "Epoch = 0 loss =  12.932663917541504\n",
            "Epoch = 0 loss =  10.651195526123047\n",
            "Epoch = 0 loss =  9.932064056396484\n",
            "Epoch = 0 loss =  5.890986442565918\n",
            "Epoch = 0 loss =  23.297481536865234\n",
            "Epoch = 0 loss =  6.916168212890625\n",
            "Epoch = 0 loss =  7.428464889526367\n",
            "Epoch = 0 loss =  5.253246307373047\n",
            "Epoch = 0 loss =  5.98457145690918\n",
            "Epoch = 0 loss =  8.036373138427734\n",
            "Epoch = 0 loss =  5.985053062438965\n",
            "Epoch = 0 loss =  4.260255813598633\n",
            "Epoch = 0 loss =  7.701005935668945\n",
            "Epoch = 0 loss =  4.164421081542969\n",
            "Epoch = 0 loss =  3.191020965576172\n",
            "Epoch = 0 loss =  5.228076934814453\n",
            "Epoch = 0 loss =  3.4177680015563965\n",
            "Epoch = 0 loss =  5.373800277709961\n",
            "Epoch = 0 loss =  3.5827693939208984\n",
            "Epoch = 0 loss =  8.054998397827148\n",
            "Epoch = 0 loss =  2.5398292541503906\n",
            "Epoch = 0 loss =  6.798454284667969\n",
            "Epoch = 1 loss =  4.421871185302734\n",
            "Epoch = 1 loss =  3.2689361572265625\n",
            "Epoch = 1 loss =  3.494442939758301\n",
            "Epoch = 1 loss =  7.500680923461914\n",
            "Epoch = 1 loss =  2.5575523376464844\n",
            "Epoch = 1 loss =  4.09588623046875\n",
            "Epoch = 1 loss =  5.753330230712891\n",
            "Epoch = 1 loss =  2.598682403564453\n",
            "Epoch = 1 loss =  3.1904983520507812\n",
            "Epoch = 1 loss =  2.625211715698242\n",
            "Epoch = 1 loss =  3.137478828430176\n",
            "Epoch = 1 loss =  8.28593635559082\n",
            "Epoch = 1 loss =  2.1462650299072266\n",
            "Epoch = 1 loss =  2.3680953979492188\n",
            "Epoch = 1 loss =  7.244171142578125\n",
            "Epoch = 1 loss =  2.2041397094726562\n",
            "Epoch = 1 loss =  8.392105102539062\n",
            "Epoch = 1 loss =  1.2861328125\n",
            "Epoch = 1 loss =  2.736330032348633\n",
            "Epoch = 1 loss =  2.83782958984375\n",
            "Epoch = 1 loss =  3.2169113159179688\n",
            "Epoch = 1 loss =  1.6121625900268555\n",
            "Epoch = 1 loss =  6.458505630493164\n",
            "Epoch = 1 loss =  3.2899513244628906\n",
            "Epoch = 1 loss =  2.1493186950683594\n",
            "Epoch = 1 loss =  2.679302215576172\n",
            "Epoch = 1 loss =  1.4758176803588867\n",
            "Epoch = 1 loss =  4.682075500488281\n",
            "Epoch = 1 loss =  1.3310956954956055\n",
            "Epoch = 1 loss =  1.5639495849609375\n",
            "Epoch = 1 loss =  1.0092964172363281\n",
            "Epoch = 1 loss =  1.3695793151855469\n",
            "Epoch = 1 loss =  1.886861801147461\n",
            "Epoch = 1 loss =  1.52471923828125\n",
            "Epoch = 1 loss =  0.9451737403869629\n",
            "Epoch = 1 loss =  2.2028419971466064\n",
            "Epoch = 1 loss =  1.7067680358886719\n",
            "Epoch = 1 loss =  4.368967056274414\n",
            "Epoch = 1 loss =  1.4428520202636719\n",
            "Epoch = 1 loss =  2.731159210205078\n",
            "Epoch = 2 loss =  1.9705867767333984\n",
            "Epoch = 2 loss =  1.656656265258789\n",
            "Epoch = 2 loss =  2.712657928466797\n",
            "Epoch = 2 loss =  5.6244659423828125\n",
            "Epoch = 2 loss =  1.377584457397461\n",
            "Epoch = 2 loss =  3.288379669189453\n",
            "Epoch = 2 loss =  2.871580123901367\n",
            "Epoch = 2 loss =  1.5712471008300781\n",
            "Epoch = 2 loss =  1.6580677032470703\n",
            "Epoch = 2 loss =  1.2966690063476562\n",
            "Epoch = 2 loss =  2.0345869064331055\n",
            "Epoch = 2 loss =  7.793634414672852\n",
            "Epoch = 2 loss =  1.24749755859375\n",
            "Epoch = 2 loss =  1.1880378723144531\n",
            "Epoch = 2 loss =  3.826406478881836\n",
            "Epoch = 2 loss =  2.4740028381347656\n",
            "Epoch = 2 loss =  5.639617919921875\n",
            "Epoch = 2 loss =  1.0188469886779785\n",
            "Epoch = 2 loss =  2.18106746673584\n",
            "Epoch = 2 loss =  2.36871337890625\n",
            "Epoch = 2 loss =  1.9164848327636719\n",
            "Epoch = 2 loss =  0.9629137516021729\n",
            "Epoch = 2 loss =  3.987821578979492\n",
            "Epoch = 2 loss =  2.4815139770507812\n",
            "Epoch = 2 loss =  0.9971199035644531\n",
            "Epoch = 2 loss =  0.6126174926757812\n",
            "Epoch = 2 loss =  0.7961583137512207\n",
            "Epoch = 2 loss =  2.8721923828125\n",
            "Epoch = 2 loss =  0.6169795989990234\n",
            "Epoch = 2 loss =  0.7410202026367188\n",
            "Epoch = 2 loss =  0.88702392578125\n",
            "Epoch = 2 loss =  0.3403205871582031\n",
            "Epoch = 2 loss =  1.6341934204101562\n",
            "Epoch = 2 loss =  1.2110557556152344\n",
            "Epoch = 2 loss =  0.7893428802490234\n",
            "Epoch = 2 loss =  0.7900233268737793\n",
            "Epoch = 2 loss =  0.9768352508544922\n",
            "Epoch = 2 loss =  4.129779815673828\n",
            "Epoch = 2 loss =  0.7379226684570312\n",
            "Epoch = 2 loss =  1.4995651245117188\n",
            "Epoch = 3 loss =  1.7327232360839844\n",
            "Epoch = 3 loss =  1.047769546508789\n",
            "Epoch = 3 loss =  2.1886959075927734\n",
            "Epoch = 3 loss =  1.7339401245117188\n",
            "Epoch = 3 loss =  1.1616401672363281\n",
            "Epoch = 3 loss =  2.772308349609375\n",
            "Epoch = 3 loss =  0.8637199401855469\n",
            "Epoch = 3 loss =  0.8404598236083984\n",
            "Epoch = 3 loss =  1.720560073852539\n",
            "Epoch = 3 loss =  0.47348785400390625\n",
            "Epoch = 3 loss =  1.5160045623779297\n",
            "Epoch = 3 loss =  6.497352600097656\n",
            "Epoch = 3 loss =  0.9966545104980469\n",
            "Epoch = 3 loss =  0.7146148681640625\n",
            "Epoch = 3 loss =  2.8708324432373047\n",
            "Epoch = 3 loss =  1.2994556427001953\n",
            "Epoch = 3 loss =  3.7347335815429688\n",
            "Epoch = 3 loss =  0.3453636169433594\n",
            "Epoch = 3 loss =  2.272679328918457\n",
            "Epoch = 3 loss =  1.1488189697265625\n",
            "Epoch = 3 loss =  1.3109855651855469\n",
            "Epoch = 3 loss =  0.7175216674804688\n",
            "Epoch = 3 loss =  3.8981781005859375\n",
            "Epoch = 3 loss =  2.612987518310547\n",
            "Epoch = 3 loss =  0.5928421020507812\n",
            "Epoch = 3 loss =  0.2917823791503906\n",
            "Epoch = 3 loss =  0.6048312187194824\n",
            "Epoch = 3 loss =  3.393871307373047\n",
            "Epoch = 3 loss =  0.3861846923828125\n",
            "Epoch = 3 loss =  0.8262252807617188\n",
            "Epoch = 3 loss =  0.45384979248046875\n",
            "Epoch = 3 loss =  0.22486495971679688\n",
            "Epoch = 3 loss =  1.6390209197998047\n",
            "Epoch = 3 loss =  0.485107421875\n",
            "Epoch = 3 loss =  0.3647184371948242\n",
            "Epoch = 3 loss =  0.5594463348388672\n",
            "Epoch = 3 loss =  0.26812171936035156\n",
            "Epoch = 3 loss =  2.282459259033203\n",
            "Epoch = 3 loss =  0.4263954162597656\n",
            "Epoch = 3 loss =  1.3471298217773438\n",
            "Epoch = 4 loss =  1.0726642608642578\n",
            "Epoch = 4 loss =  0.44064903259277344\n",
            "Epoch = 4 loss =  1.9461307525634766\n",
            "Epoch = 4 loss =  1.2747955322265625\n",
            "Epoch = 4 loss =  0.16687393188476562\n",
            "Epoch = 4 loss =  0.6004905700683594\n",
            "Epoch = 4 loss =  1.12591552734375\n",
            "Epoch = 4 loss =  0.9113903045654297\n",
            "Epoch = 4 loss =  1.2107429504394531\n",
            "Epoch = 4 loss =  0.6029071807861328\n",
            "Epoch = 4 loss =  0.6305885314941406\n",
            "Epoch = 4 loss =  4.405052185058594\n",
            "Epoch = 4 loss =  0.7658500671386719\n",
            "Epoch = 4 loss =  0.4963188171386719\n",
            "Epoch = 4 loss =  1.942403793334961\n",
            "Epoch = 4 loss =  0.4723834991455078\n",
            "Epoch = 4 loss =  2.8961334228515625\n",
            "Epoch = 4 loss =  1.2470932006835938\n",
            "Epoch = 4 loss =  1.8275556564331055\n",
            "Epoch = 4 loss =  0.48846435546875\n",
            "Epoch = 4 loss =  0.542236328125\n",
            "Epoch = 4 loss =  0.3971400260925293\n",
            "Epoch = 4 loss =  3.0236034393310547\n",
            "Epoch = 4 loss =  2.47161865234375\n",
            "Epoch = 4 loss =  0.4876594543457031\n",
            "Epoch = 4 loss =  0.23444747924804688\n",
            "Epoch = 4 loss =  0.38308000564575195\n",
            "Epoch = 4 loss =  2.646381378173828\n",
            "Epoch = 4 loss =  0.2109355926513672\n",
            "Epoch = 4 loss =  0.2177591323852539\n",
            "Epoch = 4 loss =  0.35449981689453125\n",
            "Epoch = 4 loss =  0.18773269653320312\n",
            "Epoch = 4 loss =  1.523284912109375\n",
            "Epoch = 4 loss =  0.5744552612304688\n",
            "Epoch = 4 loss =  0.2601318359375\n",
            "Epoch = 4 loss =  0.635866641998291\n",
            "Epoch = 4 loss =  0.24996185302734375\n",
            "Epoch = 4 loss =  0.417388916015625\n",
            "Epoch = 4 loss =  0.669036865234375\n",
            "Epoch = 4 loss =  1.1295661926269531\n",
            "Epoch = 5 loss =  1.9183731079101562\n",
            "Epoch = 5 loss =  0.210174560546875\n",
            "Epoch = 5 loss =  2.036334991455078\n",
            "Epoch = 5 loss =  0.8182449340820312\n",
            "Epoch = 5 loss =  0.21097946166992188\n",
            "Epoch = 5 loss =  0.4224052429199219\n",
            "Epoch = 5 loss =  0.3786468505859375\n",
            "Epoch = 5 loss =  0.36937904357910156\n",
            "Epoch = 5 loss =  0.6589508056640625\n",
            "Epoch = 5 loss =  0.4388561248779297\n",
            "Epoch = 5 loss =  0.4342832565307617\n",
            "Epoch = 5 loss =  3.2945175170898438\n",
            "Epoch = 5 loss =  0.5290870666503906\n",
            "Epoch = 5 loss =  0.3008918762207031\n",
            "Epoch = 5 loss =  1.335714340209961\n",
            "Epoch = 5 loss =  0.22357177734375\n",
            "Epoch = 5 loss =  2.9912872314453125\n",
            "Epoch = 5 loss =  0.7679424285888672\n",
            "Epoch = 5 loss =  1.6577844619750977\n",
            "Epoch = 5 loss =  1.7692451477050781\n",
            "Epoch = 5 loss =  0.2834930419921875\n",
            "Epoch = 5 loss =  0.4522542953491211\n",
            "Epoch = 5 loss =  2.4940872192382812\n",
            "Epoch = 5 loss =  2.8445396423339844\n",
            "Epoch = 5 loss =  0.2903594970703125\n",
            "Epoch = 5 loss =  0.15953445434570312\n",
            "Epoch = 5 loss =  0.6022543907165527\n",
            "Epoch = 5 loss =  2.4475555419921875\n",
            "Epoch = 5 loss =  0.13463783264160156\n",
            "Epoch = 5 loss =  0.19367599487304688\n",
            "Epoch = 5 loss =  0.3720970153808594\n",
            "Epoch = 5 loss =  0.27100372314453125\n",
            "Epoch = 5 loss =  1.180572509765625\n",
            "Epoch = 5 loss =  0.552093505859375\n",
            "Epoch = 5 loss =  0.1644277572631836\n",
            "Epoch = 5 loss =  0.303617000579834\n",
            "Epoch = 5 loss =  0.3020439147949219\n",
            "Epoch = 5 loss =  0.9815654754638672\n",
            "Epoch = 5 loss =  0.27687835693359375\n",
            "Epoch = 5 loss =  1.0521659851074219\n",
            "Epoch = 6 loss =  0.7449722290039062\n",
            "Epoch = 6 loss =  0.4861412048339844\n",
            "Epoch = 6 loss =  2.082834243774414\n",
            "Epoch = 6 loss =  3.5227317810058594\n",
            "Epoch = 6 loss =  0.44724273681640625\n",
            "Epoch = 6 loss =  0.24988937377929688\n",
            "Epoch = 6 loss =  0.24478912353515625\n",
            "Epoch = 6 loss =  0.5308074951171875\n",
            "Epoch = 6 loss =  0.4659767150878906\n",
            "Epoch = 6 loss =  0.9117317199707031\n",
            "Epoch = 6 loss =  0.6298007965087891\n",
            "Epoch = 6 loss =  1.9790973663330078\n",
            "Epoch = 6 loss =  0.4651756286621094\n",
            "Epoch = 6 loss =  0.32260894775390625\n",
            "Epoch = 6 loss =  0.8817958831787109\n",
            "Epoch = 6 loss =  0.29257774353027344\n",
            "Epoch = 6 loss =  2.3036575317382812\n",
            "Epoch = 6 loss =  0.4124908447265625\n",
            "Epoch = 6 loss =  2.0240049362182617\n",
            "Epoch = 6 loss =  1.8005523681640625\n",
            "Epoch = 6 loss =  0.24608230590820312\n",
            "Epoch = 6 loss =  0.5767240524291992\n",
            "Epoch = 6 loss =  2.1736698150634766\n",
            "Epoch = 6 loss =  2.0849876403808594\n",
            "Epoch = 6 loss =  0.39742279052734375\n",
            "Epoch = 6 loss =  0.14472198486328125\n",
            "Epoch = 6 loss =  0.5932407379150391\n",
            "Epoch = 6 loss =  1.97454833984375\n",
            "Epoch = 6 loss =  0.12709808349609375\n",
            "Epoch = 6 loss =  0.17076396942138672\n",
            "Epoch = 6 loss =  0.18916702270507812\n",
            "Epoch = 6 loss =  0.10125350952148438\n",
            "Epoch = 6 loss =  1.2291526794433594\n",
            "Epoch = 6 loss =  1.3081398010253906\n",
            "Epoch = 6 loss =  0.19620084762573242\n",
            "Epoch = 6 loss =  0.3281102180480957\n",
            "Epoch = 6 loss =  0.13566207885742188\n",
            "Epoch = 6 loss =  0.5491065979003906\n",
            "Epoch = 6 loss =  0.7351760864257812\n",
            "Epoch = 6 loss =  0.6815261840820312\n",
            "Epoch = 7 loss =  0.5884857177734375\n",
            "Epoch = 7 loss =  0.12388992309570312\n",
            "Epoch = 7 loss =  2.1346817016601562\n",
            "Epoch = 7 loss =  0.567169189453125\n",
            "Epoch = 7 loss =  0.10025405883789062\n",
            "Epoch = 7 loss =  0.25580596923828125\n",
            "Epoch = 7 loss =  0.19207000732421875\n",
            "Epoch = 7 loss =  0.13140106201171875\n",
            "Epoch = 7 loss =  0.23984336853027344\n",
            "Epoch = 7 loss =  0.12506484985351562\n",
            "Epoch = 7 loss =  0.2717113494873047\n",
            "Epoch = 7 loss =  1.3851985931396484\n",
            "Epoch = 7 loss =  0.3328590393066406\n",
            "Epoch = 7 loss =  0.23222732543945312\n",
            "Epoch = 7 loss =  0.4553794860839844\n",
            "Epoch = 7 loss =  0.09579849243164062\n",
            "Epoch = 7 loss =  2.077850341796875\n",
            "Epoch = 7 loss =  0.5949850082397461\n",
            "Epoch = 7 loss =  1.3400630950927734\n",
            "Epoch = 7 loss =  0.4626312255859375\n",
            "Epoch = 7 loss =  0.713470458984375\n",
            "Epoch = 7 loss =  0.24348020553588867\n",
            "Epoch = 7 loss =  1.3728485107421875\n",
            "Epoch = 7 loss =  1.6224403381347656\n",
            "Epoch = 7 loss =  1.1577644348144531\n",
            "Epoch = 7 loss =  0.11603546142578125\n",
            "Epoch = 7 loss =  0.31072044372558594\n",
            "Epoch = 7 loss =  1.6803398132324219\n",
            "Epoch = 7 loss =  0.11874008178710938\n",
            "Epoch = 7 loss =  0.1240854263305664\n",
            "Epoch = 7 loss =  0.17153549194335938\n",
            "Epoch = 7 loss =  0.08583450317382812\n",
            "Epoch = 7 loss =  0.3584156036376953\n",
            "Epoch = 7 loss =  0.23747634887695312\n",
            "Epoch = 7 loss =  0.1366877555847168\n",
            "Epoch = 7 loss =  0.42984962463378906\n",
            "Epoch = 7 loss =  0.08726882934570312\n",
            "Epoch = 7 loss =  0.85565185546875\n",
            "Epoch = 7 loss =  0.08571624755859375\n",
            "Epoch = 7 loss =  0.30730438232421875\n",
            "Epoch = 8 loss =  0.3943901062011719\n",
            "Epoch = 8 loss =  0.0861663818359375\n",
            "Epoch = 8 loss =  1.6232547760009766\n",
            "Epoch = 8 loss =  0.5955276489257812\n",
            "Epoch = 8 loss =  0.32947540283203125\n",
            "Epoch = 8 loss =  0.2642822265625\n",
            "Epoch = 8 loss =  0.12734603881835938\n",
            "Epoch = 8 loss =  0.2173919677734375\n",
            "Epoch = 8 loss =  0.16183090209960938\n",
            "Epoch = 8 loss =  0.09190750122070312\n",
            "Epoch = 8 loss =  0.10567092895507812\n",
            "Epoch = 8 loss =  0.22095298767089844\n",
            "Epoch = 8 loss =  0.1622161865234375\n",
            "Epoch = 8 loss =  0.11685943603515625\n",
            "Epoch = 8 loss =  0.4847221374511719\n",
            "Epoch = 8 loss =  0.06101799011230469\n",
            "Epoch = 8 loss =  0.6480331420898438\n",
            "Epoch = 8 loss =  0.11075305938720703\n",
            "Epoch = 8 loss =  1.0867338180541992\n",
            "Epoch = 8 loss =  0.3198966979980469\n",
            "Epoch = 8 loss =  0.11655044555664062\n",
            "Epoch = 8 loss =  0.13161087036132812\n",
            "Epoch = 8 loss =  0.3583106994628906\n",
            "Epoch = 8 loss =  1.5133705139160156\n",
            "Epoch = 8 loss =  0.23354721069335938\n",
            "Epoch = 8 loss =  0.08668899536132812\n",
            "Epoch = 8 loss =  0.10063791275024414\n",
            "Epoch = 8 loss =  0.7346611022949219\n",
            "Epoch = 8 loss =  0.07647514343261719\n",
            "Epoch = 8 loss =  0.09607696533203125\n",
            "Epoch = 8 loss =  0.3314628601074219\n",
            "Epoch = 8 loss =  0.08766555786132812\n",
            "Epoch = 8 loss =  0.18241500854492188\n",
            "Epoch = 8 loss =  0.20550537109375\n",
            "Epoch = 8 loss =  0.0901484489440918\n",
            "Epoch = 8 loss =  0.2216777801513672\n",
            "Epoch = 8 loss =  0.07497024536132812\n",
            "Epoch = 8 loss =  0.4364604949951172\n",
            "Epoch = 8 loss =  0.37647247314453125\n",
            "Epoch = 8 loss =  0.13871383666992188\n",
            "Epoch = 9 loss =  0.4030609130859375\n",
            "Epoch = 9 loss =  0.07494354248046875\n",
            "Epoch = 9 loss =  1.0850906372070312\n",
            "Epoch = 9 loss =  0.6549339294433594\n",
            "Epoch = 9 loss =  0.2059783935546875\n",
            "Epoch = 9 loss =  0.17218017578125\n",
            "Epoch = 9 loss =  0.46401214599609375\n",
            "Epoch = 9 loss =  0.09595108032226562\n",
            "Epoch = 9 loss =  0.059268951416015625\n",
            "Epoch = 9 loss =  0.08850860595703125\n",
            "Epoch = 9 loss =  0.2109203338623047\n",
            "Epoch = 9 loss =  0.32836341857910156\n",
            "Epoch = 9 loss =  0.13942337036132812\n",
            "Epoch = 9 loss =  0.3900871276855469\n",
            "Epoch = 9 loss =  0.22809219360351562\n",
            "Epoch = 9 loss =  0.09874916076660156\n",
            "Epoch = 9 loss =  0.7596893310546875\n",
            "Epoch = 9 loss =  0.17676448822021484\n",
            "Epoch = 9 loss =  0.5930109024047852\n",
            "Epoch = 9 loss =  0.12623977661132812\n",
            "Epoch = 9 loss =  0.10399627685546875\n",
            "Epoch = 9 loss =  0.06617546081542969\n",
            "Epoch = 9 loss =  0.2997322082519531\n",
            "Epoch = 9 loss =  1.053863525390625\n",
            "Epoch = 9 loss =  0.4244384765625\n",
            "Epoch = 9 loss =  0.07233428955078125\n",
            "Epoch = 9 loss =  0.06904363632202148\n",
            "Epoch = 9 loss =  0.2824859619140625\n",
            "Epoch = 9 loss =  0.06916236877441406\n",
            "Epoch = 9 loss =  0.12833118438720703\n",
            "Epoch = 9 loss =  0.09816360473632812\n",
            "Epoch = 9 loss =  0.059539794921875\n",
            "Epoch = 9 loss =  0.06934165954589844\n",
            "Epoch = 9 loss =  0.3646240234375\n",
            "Epoch = 9 loss =  0.05570650100708008\n",
            "Epoch = 9 loss =  0.1349925994873047\n",
            "Epoch = 9 loss =  1.087860107421875\n",
            "Epoch = 9 loss =  0.06978034973144531\n",
            "Epoch = 9 loss =  0.056797027587890625\n",
            "Epoch = 9 loss =  0.16350936889648438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYTAPcCBfKm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02a00fb9-add2-4953-aed5-7d6c6fe0b347"
      },
      "cell_type": "code",
      "source": [
        "valid_trX = pad_sequence([torch.Tensor(v) for v in valid_indexed_tokens],batch_first = True).cuda().long()\n",
        "valid_trY = pad_sequence([torch.Tensor(v) for v in valid_labels_idx],batch_first = True).cuda().long()\n",
        "\n",
        "with torch.no_grad():\n",
        "    bert.eval()\n",
        "    segments_tensors = torch.zeros(valid_trX.size()).cuda().long()\n",
        "    attention_mask = (valid_trX != 0).cuda().long()\n",
        "    encoded_bert, _ = bert(valid_trX, segments_tensors,attention_mask,output_all_encoded_layers=False)\n",
        "    \n",
        "    pos = linear(encoded_bert)\n",
        "    valid_trY = valid_trY\n",
        "    pred_trY = crf.decode(pos,(valid_trY!=0).float())\n",
        "    \n",
        "print (\"Acc = \",(valid_trY[valid_trY!=0] == pred_trY[valid_trY!=0]).float().mean())"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc =  tensor(0.9799, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M_5g6ej9o7aU",
        "colab_type": "code",
        "outputId": "4823877b-df53-4f7d-aae5-866324fd04dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1799
        }
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(pred_trY.view(9,-1).cpu().numpy()).applymap(lambda x: idx2pos[x]).loc[1],pd.DataFrame(valid_trY.view(9,-1).cpu().numpy()).applymap(lambda x: idx2pos[x]).loc[1]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0     [START]\n",
              " 1          ''\n",
              " 2          ''\n",
              " 3          DT\n",
              " 4          CD\n",
              " 5         NNS\n",
              " 6         VBP\n",
              " 7          JJ\n",
              " 8         NNS\n",
              " 9          IN\n",
              " 10         DT\n",
              " 11         JJ\n",
              " 12         NN\n",
              " 13          :\n",
              " 14          :\n",
              " 15        NNP\n",
              " 16        NNP\n",
              " 17        NNP\n",
              " 18        NNP\n",
              " 19        VBZ\n",
              " 20         DT\n",
              " 21         JJ\n",
              " 22         CC\n",
              " 23        NNP\n",
              " 24        NNP\n",
              " 25        NNP\n",
              " 26        VBZ\n",
              " 27         DT\n",
              " 28         NN\n",
              " 29          .\n",
              " 30     [STOP]\n",
              " 31     [STOP]\n",
              " 32     [STOP]\n",
              " 33     [STOP]\n",
              " 34     [STOP]\n",
              " 35     [STOP]\n",
              " 36     [STOP]\n",
              " 37     [STOP]\n",
              " 38     [STOP]\n",
              " 39     [STOP]\n",
              " 40     [STOP]\n",
              " 41     [STOP]\n",
              " 42     [STOP]\n",
              " 43     [STOP]\n",
              " 44     [STOP]\n",
              " 45     [STOP]\n",
              " 46     [STOP]\n",
              " 47     [STOP]\n",
              " 48     [STOP]\n",
              " Name: 1, dtype: object, 0     [START]\n",
              " 1          ''\n",
              " 2           O\n",
              " 3          DT\n",
              " 4          CD\n",
              " 5         NNS\n",
              " 6         VBP\n",
              " 7          JJ\n",
              " 8         NNS\n",
              " 9          IN\n",
              " 10         DT\n",
              " 11         JJ\n",
              " 12         NN\n",
              " 13          :\n",
              " 14          O\n",
              " 15        NNP\n",
              " 16          O\n",
              " 17        NNP\n",
              " 18          O\n",
              " 19        VBZ\n",
              " 20         DT\n",
              " 21         JJ\n",
              " 22         CC\n",
              " 23        NNP\n",
              " 24          O\n",
              " 25        NNP\n",
              " 26        VBZ\n",
              " 27         DT\n",
              " 28         NN\n",
              " 29          .\n",
              " 30     [STOP]\n",
              " 31          O\n",
              " 32          O\n",
              " 33          O\n",
              " 34          O\n",
              " 35          O\n",
              " 36          O\n",
              " 37          O\n",
              " 38          O\n",
              " 39          O\n",
              " 40          O\n",
              " 41          O\n",
              " 42          O\n",
              " 43          O\n",
              " 44          O\n",
              " 45          O\n",
              " 46          O\n",
              " 47          O\n",
              " 48          O\n",
              " Name: 1, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "0x2DvFiA64GU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "59a2aeab-7b5b-4bd5-cc61-5737d042774b"
      },
      "cell_type": "code",
      "source": [
        "valid_trY[valid_trY!=0] == pred_trY[valid_trY!=0]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "P-fAVT5RhsfZ",
        "colab_type": "code",
        "outputId": "04203336-cfcb-456f-fdea-54a7ec22f22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "pred_trY[valid_trY!=0]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3, 45, 31, 10, 32, 16, 30, 12,  1, 31, 35, 22, 36, 36, 36, 28, 36, 30,\n",
              "        36,  9, 30, 37, 22, 31, 32, 35, 22, 31, 35,  7, 47, 31, 32, 35,  1, 19,\n",
              "        31, 32, 35, 40, 17,  3, 42, 31, 10, 16, 44, 32, 16, 22, 31, 32, 35, 20,\n",
              "        36, 36, 47, 31, 32, 45, 36, 36, 47, 31, 32, 40, 17,  3, 31, 10, 16, 37,\n",
              "        31, 36, 47, 31, 35,  1, 19, 32, 16, 30, 14, 32, 16, 40, 17,  3, 14, 30,\n",
              "        38, 37, 31, 32, 16, 35, 37, 10, 16, 22, 37, 32, 32, 16, 30, 45, 15, 37,\n",
              "        31, 35, 22, 31, 32, 35, 40, 17,  3, 38, 14, 37, 22, 33, 22, 31, 35, 16,\n",
              "        44, 13, 22, 32, 35, 35, 22, 31, 32, 35, 30, 45,  7, 31, 44, 13, 31, 35,\n",
              "        22, 32, 35, 35, 47, 22, 31, 35, 42, 22, 31, 35, 40, 17,  3, 36, 36, 37,\n",
              "        22, 31, 32, 35, 22, 38, 47, 35,  1, 19, 31, 35, 32, 35, 35, 30, 45, 22,\n",
              "        38, 29, 19, 31,  7, 32, 35, 22, 35, 42, 22, 36, 36,  1, 19, 31, 35, 22,\n",
              "        32, 35, 40, 17,  3, 36, 41, 36, 37, 38, 37, 31, 32, 35,  1, 19, 10, 35,\n",
              "        16,  1, 35, 36, 36, 22, 36, 40, 17,  3, 16, 37, 14, 13, 40, 17,  3, 36,\n",
              "        37, 38, 47,  1, 19, 35, 22, 31, 32, 35, 22, 32, 35, 40, 17],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "uyhcKQpRz2Km",
        "colab_type": "code",
        "outputId": "c2cf99a9-ee7b-4c45-baf7-b4a608f816d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "  for j in range(49):\n",
        "    if valid_trY[i,j] != 0:\n",
        "      if valid_trY[i,j] != pred_trY[i,j]:\n",
        "        print (i,j)\n",
        "        print (pred_trY[i,j],valid_trY[i,j])\n",
        "  "
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 9\n",
            "tensor(37, device='cuda:0') tensor(13, device='cuda:0')\n",
            "3 12\n",
            "tensor(14, device='cuda:0') tensor(22, device='cuda:0')\n",
            "4 20\n",
            "tensor(32, device='cuda:0') tensor(36, device='cuda:0')\n",
            "4 21\n",
            "tensor(35, device='cuda:0') tensor(36, device='cuda:0')\n",
            "4 33\n",
            "tensor(35, device='cuda:0') tensor(32, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QpR0O5zypkqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0e116d3f-50ee-4f3c-fe1b-efff19bdfe07"
      },
      "cell_type": "code",
      "source": [
        "valid_trY[1,28:]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35, 40, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "Stt9wJX6qCrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8af552ca-ffdc-48f6-b625-813a718a0190"
      },
      "cell_type": "code",
      "source": [
        "pred_trY[1,28:]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([32, 40, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
              "        17, 17, 17], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "0sk5ODxfqfD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}